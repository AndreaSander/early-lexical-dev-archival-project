group_by(studio_test_name, eng_exp, fre_exp)%>%
mutate(trial_lang = case_when(str_detect(studio_test_name,"E")~ "english",
str_detect(studio_test_name, "F")~ "french"))
#centering to an easy to interpret value that is close to the mean, 25 months for age and 50% for experience.
arch_re_zero<- arch_re_zero%>%
mutate(exp_to_target_lang = case_when(trial_lang == "english" ~ eng_exp,
trial_lang == "french" ~ fre_exp)) %>%
mutate(exp_target_lang_prop = exp_to_target_lang/100)
#mean(arch_re_zero$age_months)#25.79
#mean(arch_re_zero$exp_target_lang_prop) #0.57
arch_re_zero_c<- arch_re_zero %>%
mutate(age_centered = age_months-25) %>%
mutate(exp_centered =exp_target_lang_prop - .50)
##-------------------Data preparation for GAMs and GLMER (LMMs)-------------------------------------##
for_lmm <- arch_re_zero_c
for_lmm <- for_lmm %>%
filter(noun_onset>= 360 & noun_onset <= 3000)%>%
group_by(recording_name, subject_id, trial_number, media_name, age_centered,exp_centered)%>%
summarise(samples_total=sum(target==TRUE, target==FALSE ),
samples_target=sum(target))%>%
mutate(prop_looking= samples_target/samples_total)
#age as a smooth (non-linear) effect. No interaction effect
gam1<- gamm4(prop_looking ~ s(age_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam1$gam)
summary(gam1$mer)
#Experience as a smooth (non-linear) effect. No interaction effect
gam2<- gamm4(prop_looking ~ s(exp_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam2$gam)
summary(gam2$mer)
#Addition of age and experience as smooth effects
gam3<- gamm4(prop_looking ~ s(exp_centered) + s(age_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam3$gam)
summary(gam3$mer)
#Since age does not have a linear distribution, plotting the empirical distribution of the funtion
plotdist(gam1$gam)
#Finding the distribution that best fits the age variable in the maturation only GAM
gam1_predict<- predict(gam1$gam)
#Since age does not have a linear distribution, plotting the empirical distribution of the funtion
plotdist(gam1_predict)
#Since age does not have a linear distribution, plotting the empirical distribution of the function
plotdist(for_lmm$age_centered)
#Since age does not have a linear distribution, plotting the empirical distribution of the function
plotdist(for_lmm$age_centered, histo = T, demp = T)
descdist(for_lmm$age_centered)
descdist(for_lmm$age_centered, boot=1000)
descdist(for_lmm$exp_centered, boot=1000)
descdist(for_lmm$prop_looking, boot=1000)
#AGE CENTERED
plotdist(for_lmm$age_centered, histo = T, demp = T)
descdist(for_lmm$age_centered, boot=1000)
#EXPERIENCE CENTERED
plotdist(for_lmm$exp_centered, histo = T, demp = T)
descdist(for_lmm$exp_centered, boot=1000)
#DEPENDENT VARIABLE
plotdist(for_lmm$prop_looking, histo = T, demp = T)
descdist(for_lmm$prop_looking, boot=1000)
knitr::opts_chunk$set(echo = TRUE)
library(fitdistrplus)
library(MASS)
library(tidyverse)
library(here)
library(dplyr)
library(lme4)
library(lmerTest)
library(afex)
library(gazer)
library(mgcv)
library(gamm4)
library(broom)
library(rempsyc)
load(here("merged_data_sets/arch_final.Rda"))
arch_re_zero<- arch_final%>%
group_by(studio_project_name, studio_test_name, trial_number, recording_name)%>%
mutate(trial_from_zero = recording_timestamp-min(recording_timestamp))%>%
group_by(studio_project_name)%>%
mutate(noun_onset = case_when(studio_project_name=="CompMix-36"~trial_from_zero-3000,
studio_project_name=="LearnMix-36"~trial_from_zero-4500,
studio_project_name=="Mix-20"~trial_from_zero-5400,
studio_project_name=="Mix-14"~trial_from_zero-5400,
studio_project_name=="CogMisp-24"~trial_from_zero-1500))%>%
ungroup()%>%
rename(target_side=target, distractor_side=distractor,
gaze_point_x= gaze_point_x_adc_spx,
gaze_point_y = gaze_point_y_adc_spx)%>%
filter(gaze_point_x>=0 & gaze_point_x<= 1920)%>% #keeps only observations that are in the screen
filter(gaze_point_y>=0 & gaze_point_y<=1200)%>%
filter(!is.na(gaze_point_x))%>% #gets rid of the observations where tobii didn't get any reading
filter(!is.na(gaze_point_y))%>%
filter(!is.na(validity_left)) %>%
filter(!is.na(validity_right)) %>%
filter(validity_left<= 1)%>%
filter(validity_right <= 1)%>%
mutate(target = case_when(gaze_point_x >= target_x_min&gaze_point_x <= target_x_max&gaze_point_y >= target_y_min&gaze_point_y <= target_y_max~TRUE,
TRUE~FALSE))
arch_re_zero <- arch_re_zero%>%
ungroup()%>%
group_by(studio_test_name, eng_exp, fre_exp)%>%
mutate(trial_lang = case_when(str_detect(studio_test_name,"E")~ "english",
str_detect(studio_test_name, "F")~ "french"))
#centering to an easy to interpret value that is close to the mean, 25 months for age and 50% for experience.
arch_re_zero<- arch_re_zero%>%
mutate(exp_to_target_lang = case_when(trial_lang == "english" ~ eng_exp,
trial_lang == "french" ~ fre_exp)) %>%
mutate(exp_target_lang_prop = exp_to_target_lang/100)
#mean(arch_re_zero$age_months)#25.79
#mean(arch_re_zero$exp_target_lang_prop) #0.57
arch_re_zero_c<- arch_re_zero %>%
mutate(age_centered = age_months-25) %>%
mutate(exp_centered =exp_target_lang_prop - .50)
for_gca <- arch_re_zero_c%>%
filter(noun_onset>=360 & noun_onset<=2500) %>%
mutate(time_bins= ifelse(ceiling(noun_onset/100) == 0, 100, ceiling(noun_onset/100)*100))%>%
ungroup()%>%
group_by(recording_name, subject_id, trial_number, media_name,age_centered, exp_centered, time_bins)%>%
summarise(samples_total=sum(target==TRUE, target==FALSE ),
samples_target=sum(target))%>%
mutate(prop_looking= samples_target/samples_total )
for_gca <- code_poly(for_gca, predictor = "time_bins", poly.order=3, draw.poly = FALSE)
gca_age_only <- lmer (prop_looking ~ poly1*age_centered + poly2*age_centered + (1|subject_id) + (1|media_name), data = for_gca)
summary(gca_age_only)
library(fitdistrplus)
library(MASS)
library(tidyverse)
library(here)
library(dplyr)
library(lme4)
library(lmerTest)
library(afex)
library(gazer)
library(mgcv)
library(gamm4)
library(broom)
library(rempsyc)
load(here("merged_data_sets/arch_final.Rda"))
arch_re_zero<- arch_final%>%
group_by(studio_project_name, studio_test_name, trial_number, recording_name)%>%
mutate(trial_from_zero = recording_timestamp-min(recording_timestamp))%>%
group_by(studio_project_name)%>%
mutate(noun_onset = case_when(studio_project_name=="CompMix-36"~trial_from_zero-3000,
studio_project_name=="LearnMix-36"~trial_from_zero-4500,
studio_project_name=="Mix-20"~trial_from_zero-5400,
studio_project_name=="Mix-14"~trial_from_zero-5400,
studio_project_name=="CogMisp-24"~trial_from_zero-1500))%>%
ungroup()%>%
rename(target_side=target, distractor_side=distractor,
gaze_point_x= gaze_point_x_adc_spx,
gaze_point_y = gaze_point_y_adc_spx)%>%
filter(gaze_point_x>=0 & gaze_point_x<= 1920)%>% #keeps only observations that are in the screen
filter(gaze_point_y>=0 & gaze_point_y<=1200)%>%
filter(!is.na(gaze_point_x))%>% #gets rid of the observations where tobii didn't get any reading
filter(!is.na(gaze_point_y))%>%
filter(!is.na(validity_left)) %>%
filter(!is.na(validity_right)) %>%
filter(validity_left<= 1)%>%
filter(validity_right <= 1)%>%
mutate(target = case_when(gaze_point_x >= target_x_min&gaze_point_x <= target_x_max&gaze_point_y >= target_y_min&gaze_point_y <= target_y_max~TRUE,
TRUE~FALSE))
arch_re_zero <- arch_re_zero%>%
ungroup()%>%
group_by(studio_test_name, eng_exp, fre_exp)%>%
mutate(trial_lang = case_when(str_detect(studio_test_name,"E")~ "english",
str_detect(studio_test_name, "F")~ "french"))
#centering to an easy to interpret value that is close to the mean, 25 months for age and 50% for experience.
arch_re_zero<- arch_re_zero%>%
mutate(exp_to_target_lang = case_when(trial_lang == "english" ~ eng_exp,
trial_lang == "french" ~ fre_exp)) %>%
mutate(exp_target_lang_prop = exp_to_target_lang/100)
#mean(arch_re_zero$age_months)#25.79
#mean(arch_re_zero$exp_target_lang_prop) #0.57
arch_re_zero_c<- arch_re_zero %>%
mutate(age_centered = age_months-25) %>%
mutate(exp_centered =exp_target_lang_prop - .50)
for_gca <- arch_re_zero_c%>%
filter(noun_onset>=360 & noun_onset<=2500) %>%
mutate(time_bins= ifelse(ceiling(noun_onset/100) == 0, 100, ceiling(noun_onset/100)*100))%>%
ungroup()%>%
group_by(recording_name, subject_id, trial_number, media_name,age_centered, exp_centered, time_bins)%>%
summarise(samples_total=sum(target==TRUE, target==FALSE ),
samples_target=sum(target))%>%
mutate(prop_looking= samples_target/samples_total )
for_gca <- code_poly(for_gca, predictor = "time_bins", poly.order=3, draw.poly = FALSE)
gca_age_only <- lmer (prop_looking ~ poly1*age_centered + poly2*age_centered + (1|subject_id) + (1|media_name), data = for_gca)
summary(gca_age_only)
gca_experience_only <- lmer (prop_looking ~ poly1*exp_centered + poly2*exp_centered+ (1|subject_id) + (1|media_name), data = for_gca)
summary(gca_experience_only)
gca_additive <- lmer (prop_looking ~ poly1*exp_centered + poly1*age_centered + poly2*exp_centered + poly2*age_centered + (1|subject_id) + (1|media_name), data = for_gca)
gca_accumulator <- lmer (prop_looking ~ poly1*(exp_centered*age_centered)+ poly2*(exp_centered*age_centered)+ (1|subject_id) + (1|media_name), data = for_gca)
##-------------------Data preparation for GAMs and GLMER (LMMs)-------------------------------------##
for_lmm <- arch_re_zero_c
for_lmm <- for_lmm %>%
filter(noun_onset>= 360 & noun_onset <= 3000)%>%
group_by(recording_name, subject_id, trial_number, media_name, age_centered,exp_centered)%>%
summarise(samples_total=sum(target==TRUE, target==FALSE ),
samples_target=sum(target))%>%
mutate(prop_looking= samples_target/samples_total)
#age as a smooth (non-linear) effect. No interaction effect
gam1<- gamm4(prop_looking ~ s(age_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam1$gam)
summary(gam1$mer)
#Experience as a smooth (non-linear) effect. No interaction effect
gam2<- gamm4(prop_looking ~ s(exp_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam2$gam)
summary(gam2$mer)
#Addition of age and experience as smooth effects
gam3<- gamm4(prop_looking ~ s(exp_centered) + s(age_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam3$gam)
summary(gam3$mer)
fd<-derivSimulCI?(gam1$gam)
`derivSimulCI` <- function(mod, n = 200, eps = 1e-7, newdata, term,
samples = 10000) {
stopifnot(require("MASS"))
if(inherits(mod, "gamm"))
mod <- mod$gam
m.terms <- attr(terms(mod), "term.labels")
if(missing(newdata)) {
newD <- sapply(model.frame(mod)[, m.terms, drop = FALSE],
function(x) seq(min(x), max(x) - (2*eps), length = n))
names(newD) <- m.terms
} else {
newD <- newdata
}
newDF <- data.frame(newD) ## needs to be a data frame for predict
X0 <- predict(mod, newDF, type = "lpmatrix")
newDF <- newDF + eps
X1 <- predict(mod, newDF, type = "lpmatrix")
Xp <- (X1 - X0) / eps
Xp.r <- NROW(Xp)
Xp.c <- NCOL(Xp)
## dims of bs
bs.dims <- sapply(mod$smooth, "[[", "bs.dim") - 1
## number of smooth terms
t.labs <- attr(mod$terms, "term.labels")
## match the term with the the terms in the model
if(!missing(term)) {
want <- grep(term, t.labs)
if(!identical(length(want), length(term)))
stop("One or more 'term's not found in model!")
t.labs <- t.labs[want]
}
nt <- length(t.labs)
## list to hold the derivatives
lD <- vector(mode = "list", length = nt)
names(lD) <- t.labs
## sample draws from the posterior distribution of model coefficients
Rbeta <- t(mvrnorm(n = samples, coef(mod), vcov(mod)))
## loop over the terms
for(i in seq_len(nt)) {
want <- grep(t.labs[i], colnames(X1))
lD[[i]] <- list(deriv = Xp[, want] %*% coef(mod)[want],
simulations = Xp[, want] %*% Rbeta[want, ])
}
class(lD) <- "derivSimulCI"
lD$gamModel <- mod
lD$eps <- eps
lD$eval <- newD - eps
lD ##return
}
fd<-derivSimulCI(gam1$gam)
plot(fd, sizer = TRUE)
fd<-derivSimulCI(gam1)
fd<-derivSimulCI(gam1$gam)
MASS::plot(fd, sizer = TRUE)
l <- length(x) - 3
plot.derivSimulCI <- function(x, alpha = 0.05, polygon = TRUE,
sizer = FALSE, term,
eval = 0, lwd = 3,
col = "lightgrey", border = col,
ylab, xlab, main, ...) {
l <- length(x) - 3
## get terms and check specified (if any) are in model
term.labs <- names(x[seq_len(l)])
if(missing(term)) {
term <- term.labs
} else {
term <- term.labs[match(term, term.labs)]
}
if(any(miss <- is.na(term)))
stop(paste("'term'", term[miss], "not a valid model term."))
if(all(miss))
stop("All terms in 'term' not found in model.")
l <- sum(!miss)
nplt <- n2mfrow(l)
if(missing(ylab))
ylab <- expression(italic(hat(f)*"'"*(x)))
if(missing(xlab)) {
xlab <- attr(terms(x$gamModel), "term.labels")
names(xlab) <- xlab
}
if (missing(main)) {
main <- term
names(main) <- term
}
plot.derivSimulCI(fd)
library(fitdistrplus)
library(MASS)
library(tidyverse)
library(here)
library(dplyr)
library(lme4)
library(lmerTest)
library(afex)
library(gazer)
library(mgcv)
library(gamm4)
library(broom)
library(rempsyc)
library(fitdistrplus)
library(MASS)
library(tidyverse)
library(here)
library(dplyr)
library(lme4)
library(lmerTest)
library(afex)
library(gazer)
library(mgcv)
library(gamm4)
library(broom)
library(rempsyc)
load(here("merged_data_sets/arch_final.Rda"))
arch_re_zero<- arch_final%>%
group_by(studio_project_name, studio_test_name, trial_number, recording_name)%>%
mutate(trial_from_zero = recording_timestamp-min(recording_timestamp))%>%
group_by(studio_project_name)%>%
mutate(noun_onset = case_when(studio_project_name=="CompMix-36"~trial_from_zero-3000,
studio_project_name=="LearnMix-36"~trial_from_zero-4500,
studio_project_name=="Mix-20"~trial_from_zero-5400,
studio_project_name=="Mix-14"~trial_from_zero-5400,
studio_project_name=="CogMisp-24"~trial_from_zero-1500))%>%
ungroup()%>%
rename(target_side=target, distractor_side=distractor,
gaze_point_x= gaze_point_x_adc_spx,
gaze_point_y = gaze_point_y_adc_spx)%>%
filter(gaze_point_x>=0 & gaze_point_x<= 1920)%>% #keeps only observations that are in the screen
filter(gaze_point_y>=0 & gaze_point_y<=1200)%>%
filter(!is.na(gaze_point_x))%>% #gets rid of the observations where tobii didn't get any reading
filter(!is.na(gaze_point_y))%>%
filter(!is.na(validity_left)) %>%
filter(!is.na(validity_right)) %>%
filter(validity_left<= 1)%>%
filter(validity_right <= 1)%>%
mutate(target = case_when(gaze_point_x >= target_x_min&gaze_point_x <= target_x_max&gaze_point_y >= target_y_min&gaze_point_y <= target_y_max~TRUE,
TRUE~FALSE))
arch_re_zero <- arch_re_zero%>%
ungroup()%>%
group_by(studio_test_name, eng_exp, fre_exp)%>%
mutate(trial_lang = case_when(str_detect(studio_test_name,"E")~ "english",
str_detect(studio_test_name, "F")~ "french"))
#centering to an easy to interpret value that is close to the mean, 25 months for age and 50% for experience.
arch_re_zero<- arch_re_zero%>%
mutate(exp_to_target_lang = case_when(trial_lang == "english" ~ eng_exp,
trial_lang == "french" ~ fre_exp)) %>%
mutate(exp_target_lang_prop = exp_to_target_lang/100)
#mean(arch_re_zero$age_months)#25.79
#mean(arch_re_zero$exp_target_lang_prop) #0.57
arch_re_zero_c<- arch_re_zero %>%
mutate(age_centered = age_months-25) %>%
mutate(exp_centered =exp_target_lang_prop - .50)
##-------------------Data preparation for GAMs and GLMER (LMMs)-------------------------------------##
for_lmm <- arch_re_zero_c
for_lmm <- for_lmm %>%
filter(noun_onset>= 360 & noun_onset <= 3000)%>%
group_by(recording_name, subject_id, trial_number, media_name, age_centered,exp_centered)%>%
summarise(samples_total=sum(target==TRUE, target==FALSE ),
samples_target=sum(target))%>%
mutate(prop_looking= samples_target/samples_total)
#age as a smooth (non-linear) effect. No interaction effect
gam1<- gamm4(prop_looking ~ s(age_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam1$gam)
summary(gam1$mer)
#Experience as a smooth (non-linear) effect. No interaction effect
gam2<- gamm4(prop_looking ~ s(exp_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam2$gam)
summary(gam2$mer)
#Addition of age and experience as smooth effects
gam3<- gamm4(prop_looking ~ s(exp_centered) + s(age_centered), data = for_lmm, random=~((1|media_name)+ (1|subject_id)))
summary(gam3$gam)
summary(gam3$mer)
`derivSimulCI` <- function(mod, n = 200, eps = 1e-7, newdata, term,
samples = 10000) {
stopifnot(require("MASS"))
if(inherits(mod, "gamm"))
mod <- mod$gam
m.terms <- attr(terms(mod), "term.labels")
if(missing(newdata)) {
newD <- sapply(model.frame(mod)[, m.terms, drop = FALSE],
function(x) seq(min(x), max(x) - (2*eps), length = n))
names(newD) <- m.terms
} else {
newD <- newdata
}
newDF <- data.frame(newD) ## needs to be a data frame for predict
X0 <- predict(mod, newDF, type = "lpmatrix")
newDF <- newDF + eps
X1 <- predict(mod, newDF, type = "lpmatrix")
Xp <- (X1 - X0) / eps
Xp.r <- NROW(Xp)
Xp.c <- NCOL(Xp)
## dims of bs
bs.dims <- sapply(mod$smooth, "[[", "bs.dim") - 1
## number of smooth terms
t.labs <- attr(mod$terms, "term.labels")
## match the term with the the terms in the model
if(!missing(term)) {
want <- grep(term, t.labs)
if(!identical(length(want), length(term)))
stop("One or more 'term's not found in model!")
t.labs <- t.labs[want]
}
nt <- length(t.labs)
## list to hold the derivatives
lD <- vector(mode = "list", length = nt)
names(lD) <- t.labs
## sample draws from the posterior distribution of model coefficients
Rbeta <- t(mvrnorm(n = samples, coef(mod), vcov(mod)))
## loop over the terms
for(i in seq_len(nt)) {
want <- grep(t.labs[i], colnames(X1))
lD[[i]] <- list(deriv = Xp[, want] %*% coef(mod)[want],
simulations = Xp[, want] %*% Rbeta[want, ])
}
class(lD) <- "derivSimulCI"
lD$gamModel <- mod
lD$eps <- eps
lD$eval <- newD - eps
lD ##return
}
plot.derivSimulCI <- function(x, alpha = 0.05, polygon = TRUE,
sizer = FALSE, term,
eval = 0, lwd = 3,
col = "lightgrey", border = col,
ylab, xlab, main, ...) {
l <- length(x) - 3
## get terms and check specified (if any) are in model
term.labs <- names(x[seq_len(l)])
if(missing(term)) {
term <- term.labs
} else {
term <- term.labs[match(term, term.labs)]
}
if(any(miss <- is.na(term)))
stop(paste("'term'", term[miss], "not a valid model term."))
if(all(miss))
stop("All terms in 'term' not found in model.")
l <- sum(!miss)
nplt <- n2mfrow(l)
if(missing(ylab))
ylab <- expression(italic(hat(f)*"'"*(x)))
if(missing(xlab)) {
xlab <- attr(terms(x$gamModel), "term.labels")
names(xlab) <- xlab
}
if (missing(main)) {
main <- term
names(main) <- term
}
## compute confidence interval
ciFUN <- function(x, alpha) {
ahalf <- alpha / 2
apply(x$simulations, 1, quantile, probs = c(ahalf, 1 - ahalf))
}
CI <- lapply(x[seq_len(l)], ciFUN, alpha = alpha)
## plots
layout(matrix(seq_len(l), nrow = nplt[1], ncol = nplt[2]))
on.exit(layout(1))
for(i in term) {
lwr <- CI[[i]][1,]
upr <- CI[[i]][2,]
ylim <- range(upr, lwr)
plot(x$eval[,i], x[[i]]$deriv, type = "n",
ylim = ylim, ylab = ylab, xlab = xlab[i], main = main[i], ...)
if(isTRUE(polygon)) {
polygon(c(x$eval[,i], rev(x$eval[,i])),
c(upr, rev(lwr)), col = col, border = border)
} else {
lines(x$eval[,i], upr, lty = "dashed")
lines(x$eval[,i], lwr, lty = "dashed")
}
abline(h = 0, ...)
if(isTRUE(sizer)) {
lines(x$eval[,i], x[[i]]$deriv, lwd = 1)
S <- signifD(x[[i]]$deriv, x[[i]]$deriv, upr, lwr,
eval = eval)
lines(x$eval[,i], S$incr, lwd = lwd, col = "blue")
lines(x$eval[,i], S$decr, lwd = lwd, col = "red")
} else {
lines(x$eval[,i], x[[i]]$deriv, lwd = 2)
}
}
invisible(x)
}
signifD <- function(x, d, upper, lower, eval = 0) {
miss <- upper > eval & lower < eval
incr <- decr <- x
want <- d > eval
incr[!want | miss] <- NA
want <- d < eval
decr[!want | miss] <- NA
list(incr = incr, decr = decr)
}
fd<-derivSimulCI(gam1$gam)
plot.derivSimulCI(fd, sizer = TRUE)
CI <- lapply(fd[1], function(x) t(apply(x$simulations, 1, quantile, probs = c(0.025, 0.975))))
CI <- lapply(fd[1], function(x) t(apply(x$simulations, 1, quantile, probs = c(0.025, 0.975))))
first.zero.slope.index <- min(which(sign(CI$x[, "2.5%"]) != sign(CI$x[, "97.5%"])))
CI <- lapply(fd[1], function(x) t(apply(x$simulations, 1, quantile, probs = c(0.025, 0.975))))
first.zero.slope.index <- min(which(sign(CI$x[, "2.5%"]) != sign(CI$x[, "97.5%"])))
first.zero.slope.index <- min(which(sign(CI$age_centered[, "2.5%"]) != sign(CI$age_centered[, "97.5%"])))
fd$eval[first.zero.slope.index]
plot.derivSimulCI(fd, sizer = TRUE)
fd$eval[first.zero.slope.index]
plot.derivSimulCI(fd, sizer = TRUE)
